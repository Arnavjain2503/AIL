# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q6aFL_2m9wXnCv5kS3PeU35Bakk9L_7K
"""

import nltk
nltk.download('punkt_tab')

'''Theory:
Text preprocessing is the first and crucial step in Natural Language Processing (NLP). It involves converting raw text into a clean and structured format that is suitable for analysis. This includes:

Text Cleaning: Removing unwanted characters, punctuation, special symbols, and numbers.

Tokenization: Splitting text into smaller units, such as words or sentences.

Removing Stop Words: Removing common words like "the", "is", etc., that do not contribute much to the meaning of the text.

Spelling Correction: Correcting any misspelled words to improve text consistency.

Algorithm:
Read text from the input file.

Clean the text:

Remove punctuation, special characters, and numbers using regular expressions.

Remove extra whitespaces.

Convert text to lowercase to ensure uniformity.

Tokenize the text into words.

Remove stop words using a predefined list from libraries like NLTK.

Correct misspelled words using spell correction libraries like TextBlob.

Output the cleaned, tokenized, and corrected text.

'''
# 22. Full Program
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob

nltk.download('punkt')
nltk.download('stopwords')

# Read text
with open('textfile_22.txt', 'r', encoding='utf-8') as file:
    text = file.read()

# a. Text cleaning (remove punctuation, special characters, numbers)
text = re.sub(r'[^A-Za-z\s]', '', text)
text = re.sub(r'\s+', ' ', text).strip()

# b. Convert to lowercase
text = text.lower()

# c. Tokenization
tokens = word_tokenize(text)

# d. Remove stop words
filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]

# e. Correct misspelled words
corrected_tokens = []
for word in filtered_tokens:
    corrected_word = str(TextBlob(word).correct())
    corrected_tokens.append(corrected_word)

# Display final output
print("Corrected Tokens:")
print(corrected_tokens)

'''Theory:
Stemming: The process of reducing a word to its root form by chopping off suffixes (e.g., "running" ‚Üí "run").

Lemmatization: A more advanced technique that reduces a word to its base form based on its meaning and context (e.g., "running" ‚Üí "run", but also "better" ‚Üí "good").

Algorithm:
Read the text.

Text cleaning: Remove unwanted characters and convert to lowercase.

Tokenization: Split the text into individual words.

Apply stemming to reduce words to their root form.

Apply lemmatization to find the dictionary form of the words.

Output the lemmatized tokens.
'''
# 23. Full Program
import re
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('wordnet')

# Read text
with open('textfile_23.txt', 'r', encoding='utf-8') as file:
    text = file.read()

# a. Text cleaning
text = re.sub(r'[^A-Za-z\s]', '', text)
text = re.sub(r'\s+', ' ', text).strip()

# b. Convert to lowercase
text = text.lower()

# c. Tokenization
tokens = word_tokenize(text)

# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in tokens]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]

# d. Create list of 3 consecutive words
trigrams = []
for i in range(len(lemmatized_tokens) - 2):
    trigram = ' '.join(lemmatized_tokens[i:i+3])
    trigrams.append(trigram)

# Display trigrams
print("Trigrams after Lemmatization:")
print(trigrams)

'''Theory:
One-Hot Encoding is a method of converting text data into a binary matrix where each unique word in the vocabulary is represented by a vector of length equal to the vocabulary size. If the word appears in a document, it is represented by 1, otherwise 0.

Algorithm:
Read the text files.

Extract unique words (vocabulary) from the text data.

For each document, create a binary vector based on the presence or absence of words.

Output the one-hot encoded matrix.

One-Hot Encoding (OHE) is a method of representing categorical data as binary vectors. It is a standard technique used to transform non-numeric data (like text) into a format that can be provided to machine learning algorithms.

How does One-Hot Encoding Work?
In One-Hot Encoding, each unique category (in the case of text data, each unique word) is converted into a binary vector.

The vector has the same length as the total number of unique categories (words).

For each word in the text, the position in the vector corresponding to that word is marked as 1, and all other positions are marked as 0.

Example:
Let‚Äôs say we have the following vocabulary (words in a corpus):
["cat", "dog", "fish"]

For the word "dog", the one-hot encoded vector would be:

csharp
Copy
[0, 1, 0]
'''
# 24. Full Program
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Read files
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Merge and split text into words
all_text = ' '.join(texts)
words = list(set(all_text.split()))

# Reshape for sklearn
words_array = np.array(words).reshape(-1, 1)

# One Hot Encoding
encoder = OneHotEncoder(sparse_output=False)
onehot_encoded = encoder.fit_transform(words_array)

# Display
print("Words:")
print(words)
print("\nOne Hot Encoded Matrix:")
print(onehot_encoded)

from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Merge and split text into words
all_text = ' '.join(texts)
words = list(set(all_text.split()))  # Get unique words (vocabulary)

# Initialize the OneHotEncoder (for document encoding)
encoder = OneHotEncoder(sparse_output=False)

# Split the documents into words (list of words for each document)
documents = [document.split() for document in texts]

# Create a list where each document is represented as a list of words
documents_vectorized = []

# For each document, create a list of word occurrences as one-hot encoding
for document in documents:
    # Create a binary vector where 1 means the word exists in the document, 0 means it does not
    one_hot_vector = [1 if word in document else 0 for word in words]
    documents_vectorized.append(one_hot_vector)

# Convert the result into a NumPy array for easier handling
onehot_encoded_documents = np.array(documents_vectorized)

# Create a DataFrame for better visualization
onehot_df = pd.DataFrame(onehot_encoded_documents, columns=words)

# Display the vocabulary and the one-hot encoded matrix nicely
print("Vocabulary (Words):")
print("  " + "  ".join(words))  # Print the vocabulary words horizontally

print("\nOne-Hot Encoded Matrix (Documents vs Words):")
print(onehot_df)  # Display the one-hot encoded DataFrame

from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Split all text into words and get unique words (vocabulary)
all_text = ' '.join(texts)
words = list(set(all_text.split()))  # Unique words from the combined text

# Create a mapping of word to column index
word_to_index = {word: index for index, word in enumerate(words)}

# Create a 2D array to hold the one-hot encoding matrix for all documents
onehot_encoded = []

# Loop through each document (text file)
for document in texts:
    # Initialize a list for this document
    onehot_vector = [0] * len(words)

    # Split the document into words and set the corresponding positions to 1
    for word in document.split():
        if word in word_to_index:  # Ensure the word exists in the vocabulary
            onehot_vector[word_to_index[word]] = 1

    # Append the one-hot vector of this document to the matrix
    onehot_encoded.append(onehot_vector)

# Convert the result into a NumPy array for easier handling
onehot_encoded = np.array(onehot_encoded)

# Create a DataFrame for better visualization
onehot_df = pd.DataFrame(onehot_encoded, columns=words)

# Display the vocabulary and the one-hot encoded matrix nicely
print("Vocabulary:")
print("  " + "  ".join(words))  # Print the vocabulary words horizontally

print("\nOne-Hot Encoded Matrix:")

# Loop through each document to print its one-hot encoded values
for i, onehot_vector in enumerate(onehot_encoded):
    print(f"Document {i+1}: ", end="")
    for value in onehot_vector:
        print(f"{value}", end="   ")
    print()  # Newline after each document's encoding

'''Theory:
Bag of Words (BoW) is a representation of text data where each document is represented by a vector, and each position in the vector corresponds to the frequency of a word in the document. BoW doesn't consider the order of words, only their presence.

Algorithm:
Read the text files.

Tokenize the documents.

Count the frequency of each word in the vocabulary for each document.

Output the BoW matrix where each row represents a document, and each column represents a word from the vocabulary.

What is Bag of Words (BoW)?
Bag of Words (BoW) is a representation of text data in which each document is represented as a vector. This vector‚Äôs elements are the frequencies of words (or the presence/absence) in the document.

The main idea is to treat the text as a "bag" of words, where:

The word order is ignored.

Each word is treated as a feature.

The feature values are typically the count or binary presence of the word.

How does BoW work?
Tokenization: First, the document is split into words.

Vocabulary Creation: The vocabulary is created by collecting all unique words from all documents.

Vector Creation: For each document, a vector is created where each element corresponds to a word from the vocabulary. The value in each position is either the count of that word in the document (if using count-based BoW) or 1 if the word is present in the document (if using binary BoW).

Example:
For the documents:

Document 1: "I love machine learning"

Document 2: "Machine learning is fun"

The vocabulary is:
["I", "love", "machine", "learning", "is", "fun"]

Now, let's encode these two documents:

Document 1: "I love machine learning"
BoW vector: [1, 1, 1, 1, 0, 0]

Document 2: "Machine learning is fun"
BoW vector: [0, 0, 1, 1, 1, 1]

Here, each word in the vocabulary corresponds to a column, and each document is represented by a vector where the value indicates the presence or frequency of the word.
'''
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_25_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Bag of Words
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# Convert the result to a DataFrame for better readability
bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Display the vocabulary and the BoW matrix nicely
print("Vocabulary:")
print(vectorizer.get_feature_names_out())
print("\nBag of Words Matrix:")
print(bow_df)

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the texts to get the TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

# Display the vocabulary (words from the documents)
print("Vocabulary (Words):")
print("  " + "  ".join(tfidf_vectorizer.get_feature_names_out()))  # Words printed horizontally

# Create a DataFrame for better readability
df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Display the TF-IDF matrix with documents as rows and words as columns
print("\nTF-IDF Matrix (Documents vs Words):")
print(df)

'''TF-IDF is a statistic used to evaluate the importance of a word in a document relative to a collection of documents. It is calculated by multiplying:

Term Frequency (TF): The frequency of the word in the document.

Inverse Document Frequency (IDF): The inverse of the frequency of documents containing the word. The idea is that words which occur frequently across many documents are less important.

Algorithm:
Read the text files.

Apply TF-IDF vectorization using TfidfVectorizer.

Output the TF-IDF matrix showing the importance of words across documents.

TF-IDF is a statistical measure used to evaluate the importance of a word within a document relative to a corpus (set of documents). It helps to capture the significance of a word in a document by considering both its frequency in the document and its rarity across the entire corpus.

The TF-IDF score of a word in a document is calculated as:

Term Frequency (TF): The number of times the word appears in the document.

TF
(
ùë°
,
ùëë
)
=
Number¬†of¬†times¬†term¬†t¬†appears¬†in¬†document¬†d
Total¬†number¬†of¬†terms¬†in¬†document¬†d
TF(t,d)=
Total¬†number¬†of¬†terms¬†in¬†document¬†d
Number¬†of¬†times¬†term¬†t¬†appears¬†in¬†document¬†d
‚Äã


Inverse Document Frequency (IDF): The inverse of the frequency of documents that contain the word. Words that appear in fewer documents are given higher importance.

IDF
(
ùë°
)
=
log
‚Å°
(
ùëÅ
ùëë
ùëì
(
ùë°
)
)
IDF(t)=log(
df(t)
N
‚Äã
 )

where:

N is the total number of documents.

df(t) is the number of documents containing the term t.

TF-IDF: The product of TF and IDF for each word in the document.

TF-IDF
(
ùë°
,
ùëë
)
=
TF
(
ùë°
,
ùëë
)
√ó
IDF
(
ùë°
)
TF-IDF(t,d)=TF(t,d)√óIDF(t)

How does TF-IDF work?
Term Frequency (TF) captures how often a word appears in a document. This helps in understanding the word‚Äôs relevance to the particular document.

Inverse Document Frequency (IDF) accounts for how common or rare the word is across all documents. Words that appear frequently in many documents are less informative, so their weight is reduced.

TF-IDF balances these two metrics, giving more weight to words that appear frequently in a document but are rare across the corpus.

Example:
Let‚Äôs say we have the following documents:

Document 1: "The cat sat on the mat."

Document 2: "The dog barked at the cat."

TF for "cat" in Document 1:
TF
=
1
6
=
0.1667
TF=
6
1
‚Äã
 =0.1667

IDF for "cat":
IDF
=
log
‚Å°
(
2
2
)
=
0
IDF=log(
2
2
‚Äã
 )=0

Because "cat" appears in both documents, the IDF value is 0 (not very informative).

TF-IDF for "cat":
TF-IDF
=
0.1667
√ó
0
=
0
TF-IDF=0.1667√ó0=0

If a word appears in fewer documents, its IDF score will be higher, giving it more importance.

'''
# 26. Full Program
from sklearn.feature_extraction.text import TfidfVectorizer

# Read files
texts = []
for i in range(1, 4):
    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

# Display
print("Vocabulary:")
print(tfidf_vectorizer.get_feature_names_out())
print("\nTF-IDF Matrix:")
print(tfidf_matrix.toarray())
