# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWU-VzdS08eRNks75dbVN8fQdrWR3KeH
"""

# Assignment 17 - Educational Style - No Activation - No Functions
'''Theory:
A Multi-Layer Perceptron (MLP) is a type of artificial neural network consisting of multiple layers:

Input Layer: Takes the input data (binary inputs).

Hidden Layers: Processes the input data through neurons with weights and biases. Here, we use two hidden layers.

Output Layer: Provides the final prediction (binary output).

In this experiment, there is no activation function applied to the neurons in the hidden layers and the output layer. The outputs of the neurons are calculated directly as a weighted sum of their inputs. The thresholding technique is used to convert continuous outputs to binary outputs.

Algorithm:
Initialization:

Randomly initialize the weight matrices (W1, W2, W3) and bias terms (b1, b2, b3) for each layer.

Define the learning rate (lr) and the maximum number of iterations for the training.

Training:

For each input, perform the forward pass:

Compute the output of the first hidden layer: h1_input = np.dot(x, W1) + b1.

Compute the output of the second hidden layer: h2_input = np.dot(h1_output, W2) + b2.

Compute the output layer: output_input = np.dot(h2_output, W3) + b3.

Apply thresholding to convert the output to binary: If the output is greater than or equal to the threshold, assign a value of 1; otherwise, assign 0.

Backpropagation:

If the output does not match the expected output, compute the error and propagate it back through the network:

Compute the gradient of the output layer (d_output), the second hidden layer (d_h2), and the first hidden layer (d_h1).

Update the weights using the gradients and the learning rate: W3 += lr * np.outer(h2_output, d_output), and similarly for W1 and W2.

Repeat the training until all outputs are correct or the maximum number of iterations is reached.

Final Output:

Display the final weights, biases, and the number of iterations taken.

'''
import numpy as np
from itertools import product

# Inputs
n = int(input("Enter number of binary inputs (N): "))
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter the corresponding outputs:")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

# Random initialization
np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])
threshold = round(np.random.uniform(0.1, 1.0), 1)

print("\nInitial weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("Threshold:", threshold)

learning_rate = 0.5
max_iterations = 10000
iteration = 0

while iteration < max_iterations:
    iteration += 1
    all_matched = True

    for i in range(len(input_values)):
        x = input_values[i]
        y = expected_outputs[i]

        # Forward pass manually (NO activation function)
        h1_input = np.dot(x, W1) + b1
        h1_output = h1_input

        h2_input = np.dot(h1_output, W2) + b2
        h2_output = h2_input

        output_input = np.dot(h2_output, W3) + b3
        output = output_input

        # Thresholding
        thresholded_output = 1 if output >= threshold else 0

        if thresholded_output != y:
            all_matched = False
            error = y - output

            # Backpropagation manually
            d_output = error
            d_h2 = np.dot(W3, d_output)
            d_h1 = np.dot(W2, d_h2)

            # Update weights manually
            W3 += learning_rate * np.outer(h2_output, d_output)
            W2 += learning_rate * np.outer(h1_output, d_h2)
            W1 += learning_rate * np.outer(x, d_h1)

    if all_matched:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nFinal weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("Threshold:", threshold)

# Final results
print("\nResults:")
for i in range(len(input_values)):
    x = input_values[i]
    y = expected_outputs[i]

    h1_input = np.dot(x, W1) + b1
    h1_output = h1_input
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = h2_input
    output_input = np.dot(h2_output, W3) + b3
    output = output_input

    thresholded_output = 1 if output >= threshold else 0

    print(f"\nInput: {x}")
    print(f"Expected Output: {y[0]}")
    print(f"Network Output: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

# Assignment 18 - Educational Style - No Activation - No Functions
'''Theory:
In this experiment, the MLP has 4 binary inputs, one hidden layer, and two binary outputs. The output layer has two outputs, which means the neural network is performing multi-class classification. The architecture and training mechanism are similar to the first experiment, but here we use two output units, and no activation function is used.

Algorithm:
Initialization:

Randomly initialize the weight matrices (W1, W2) and bias terms (b1, b2) for each layer.

Define the learning rate (lr) and the maximum number of iterations for the training.

Training:

For each input, perform the forward pass:

Compute the output of the hidden layer: h1_input = np.dot(x, W1) + b1.

Compute the output of the two output units: output_input = np.dot(h1_output, W2) + b2.

Thresholding:

Apply thresholding on the output to convert it to binary. If the output is greater than or equal to the threshold, set it to 1; otherwise, set it to 0.

Backpropagation:

If the output does not match the expected output, compute the error and propagate it back through the network:

Compute the gradient of the output layer (d_output) and the hidden layer (d_h1).

Update the weights using the gradients and the learning rate: W2 += lr * np.outer(h1_output, d_output) and W1 += lr * np.outer(x, d_h1).

Repeat the training until all outputs are correct or the maximum number of iterations is reached.

Final Output:

Display the final weights, biases, and the number of iterations taken.

'''
import numpy as np
from itertools import product

# Inputs
n = 4  # Fixed 4 inputs
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter two outputs for each input:")
expected_outputs = []
for inp in input_values:
    o1 = int(input(f"{inp} - Output 1: "))
    o2 = int(input(f"{inp} - Output 2: "))
    expected_outputs.append([o1, o2])
expected_outputs = np.array(expected_outputs)

# Random initialization
np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, 2))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
threshold = round(np.random.uniform(0.1, 1.0), 1)

print("\nInitial weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Threshold:", threshold)

learning_rate = 0.5
max_iterations = 10000
iteration = 0

while iteration < max_iterations:
    iteration += 1
    all_matched = True

    for i in range(len(input_values)):
        x = input_values[i]
        y = expected_outputs[i]

        # Forward pass manually (NO activation function)
        h1_input = np.dot(x, W1) + b1
        h1_output = h1_input

        output_input = np.dot(h1_output, W2) + b2
        output = output_input

        # Thresholding
        thresholded_output = (output >= threshold).astype(int)

        if not np.array_equal(thresholded_output, y):
            all_matched = False
            error = y - output

            # Backpropagation manually
            d_output = error
            d_h1 = np.dot(W2, d_output.T)

            # Update weights manually
            W2 += learning_rate * np.outer(h1_output, d_output)
            W1 += learning_rate * np.outer(x, d_h1)

    if all_matched:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nFinal weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Threshold:", threshold)

# Final results
print("\nResults:")
for i in range(len(input_values)):
    x = input_values[i]
    y = expected_outputs[i]

    h1_input = np.dot(x, W1) + b1
    h1_output = h1_input
    output_input = np.dot(h1_output, W2) + b2
    output = output_input

    thresholded_output = (output >= threshold).astype(int)

    print(f"\nInput: {x}")
    print(f"Expected Output: {y}")
    print(f"Network Output: {output}")
    print(f"Thresholded Output: {thresholded_output}")

#tambe
import numpy as np

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size, output_size):
    # Initialize weights with random values
    W1 = np.random.randn(input_size, hidden_layer_size)  # Weights for input to hidden layer
    W2 = np.random.randn(hidden_layer_size, output_size)  # Weights for hidden layer to output layer

    # Initialize biases with random values
    b1 = np.random.randn(hidden_layer_size)  # Bias for hidden layer
    b2 = np.random.randn(output_size)  # Bias for output layer

    return W1, W2, b1, b2

# Function to perform forward pass (no activation function)
def forward_pass(X, W1, W2, b1, b2):
    # First hidden layer
    Z1 = np.dot(X, W1) + b1
    # Output layer
    Z2 = np.dot(Z1, W2) + b2
    return Z1, Z2

# Function to perform training (using random input and output data)
def train_mlp(input_size, hidden_layer_size, output_size, steps=1000):
    # Initialize random weights and biases
    W1, W2, b1, b2 = initialize_weights_and_biases(input_size, hidden_layer_size, output_size)

    # Random binary inputs and outputs (for simplicity)
    X = np.random.randint(0, 2, (1, input_size))  # 1 sample with 4 binary inputs
    Y = np.random.randint(0, 2, (1, output_size))  # 2 binary outputs

    for step in range(steps):
        # Forward pass
        Z1, Z2 = forward_pass(X, W1, W2, b1, b2)

        # Displaying intermediate results every 100 steps
        if step % 100 == 0:
            print(f"Step {step}:")
            print(f"Input (X): {X}")
            print(f"Output (Y): {Y}")
            print(f"Hidden Layer Output (Z1): {Z1}")
            print(f"Output Layer (Z2): {Z2}")
            print(f"Weights (W1): {W1}")
            print(f"Weights (W2): {W2}")
            print(f"Biases (b1): {b1}")
            print(f"Biases (b2): {b2}")
            print()

    # Return final weight matrices, biases, and steps
    return W1, W2, b1, b2, steps

# Example of usage
input_size = 4  # Number of binary inputs
hidden_layer_size = 5  # Size of hidden layer
output_size = 2  # Number of binary outputs

# Train the MLP and display the results
W1, W2, b1, b2, steps = train_mlp(input_size, hidden_layer_size, output_size, steps=1000)

print(f"Final Weights (W1):\n{W1}")
print(f"Final Weights (W2):\n{W2}")
print(f"Final Biases (b1):\n{b1}")
print(f"Final Biases (b2):\n{b2}")
print(f"Total Steps: {steps}")

#19Sigmoid
'''Theory:
This experiment is similar to Experiment 1, but now we introduce the Sigmoid activation function for the neurons in both hidden layers and the output layer. The Sigmoid function squashes the output of each neuron to a value between 0 and 1. This makes the MLP suitable for binary classification tasks.

Algorithm:
Initialization:

Randomly initialize the weights and biases, as described in Experiment 1.

Training:

For each input, perform the forward pass:

Compute the input to the first hidden layer, then apply the Sigmoid function to calculate the output of the first hidden layer.

Repeat the process for the second hidden layer and the output layer.

Backpropagation:

Calculate the error for the output and propagate it back to the hidden layers, using the Sigmoid derivative during backpropagation to compute the gradients.

Update Weights:

Update the weights and biases as in Experiment 1, using the computed gradients.

Repeat until convergence or the maximum number of iterations is reached.

Final Output:

Display the final weights, biases, and the number of iterations taken.
In this experiment, we introduce the Sigmoid activation function to the network. The Sigmoid function is defined as:

Ïƒ(x)=1/1+e^âˆ’x

It outputs a value between 0 and 1, which is ideal for binary classification tasks (outputting probabilities).

'''
import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 10000
iteration = 0

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = sigmoid(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = sigmoid(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = sigmoid(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * sigmoid_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * sigmoid_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * sigmoid_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = sigmoid(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = sigmoid(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = sigmoid(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#20Relu
'''Theory:
In this experiment, we use the ReLU (Rectified Linear Unit) activation function, which is defined as f(x) = max(0, x). The ReLU activation function is commonly used in deep learning because it helps with the vanishing gradient problem. It ensures that positive values are passed through unchanged, while negative values are set to zero.

Algorithm:
Initialization:

Initialize the weights and biases randomly.

Training:

For each input, perform the forward pass:

Calculate the inputs to the hidden layers and apply the ReLU activation function to get the hidden layer outputs.

Repeat for the output layer.

Backpropagation:

Calculate the error for the output layer and propagate it back using the ReLU derivative, which is 1 for positive values and 0 for negative values.

Update Weights:

Update the weights and biases using the gradients from backpropagation.

Repeat until convergence or the maximum number of iterations is reached.

Final Output:

Display the final weights, biases, and the number of iterations taken.
In this experiment, the ReLU (Rectified Linear Unit) activation function is used. ReLU is defined as:

ð‘“(ð‘¥)=max(0,ð‘¥)
It is a very popular activation function due to its simplicity and its ability to mitigate the vanishing gradient problem. The primary reason ReLU is widely used is that it allows the network to learn faster and converge more quickly.
'''
import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 100000
iteration = 0

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = relu(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = relu(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = relu(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * relu_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * relu_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * relu_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = relu(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = relu(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = relu(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#21Tanh
'''Theory:
The Tanh (Hyperbolic Tangent) activation function is similar to the Sigmoid function, but it outputs values between -1 and 1. This makes the Tanh function suitable for both positive and negative outputs, making it especially useful for modeling data with varying signs.

Algorithm:
Initialization:

Initialize the weights and biases randomly.

Training:

For each input, perform the forward pass:

Calculate the inputs to the hidden layers and apply the Tanh activation function to get the hidden layer outputs.

Repeat for the output layer.

Backpropagation:

Calculate the error for the output layer and propagate it back using the Tanh derivative (1 - tanh(x)^2) to adjust the weights.

Update Weights:

Update the weights and biases using the gradients computed during backpropagation.

Repeat until convergence or the maximum number of iterations is reached.

Final Output:

Display the final weights, biases, and the number of iterations taken.

In this experiment, the Tanh (Hyperbolic Tangent) activation function is used, which is defined as:


tanh(x)= e^x - e^âˆ’x / e^x + e^âˆ’x

It squashes the output between -1 and 1. Unlike the Sigmoid function, which squashes the output between 0 and 1,
the Tanh function is symmetric around 0, making it ideal for modeling data with both positive and negative values.

Sigmoid is a good choice for binary classification but suffers from the vanishing gradient problem.

ReLU is faster to train and avoids the vanishing gradient problem, making it a preferred choice for deeper networks.

Tanh is beneficial when negative values are present in the data and when centered outputs are needed, but like Sigmoid, it suffers from the vanishing gradient problem.


'''
import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 10000
iteration = 0

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = tanh(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = tanh(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = tanh(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * tanh_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * tanh_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * tanh_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = tanh(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = tanh(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = tanh(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#Tambe
import numpy as np

# Tanh activation function and its derivative
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1.0 - np.tanh(x)**2

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size_1, hidden_layer_size_2, output_size):
    W1 = np.random.randn(input_size, hidden_layer_size_1)  # Weights from input to first hidden layer
    W2 = np.random.randn(hidden_layer_size_1, hidden_layer_size_2)  # Weights from first hidden to second hidden layer
    W3 = np.random.randn(hidden_layer_size_2, output_size)  # Weights from second hidden layer to output layer

    b1 = np.random.randn(hidden_layer_size_1)  # Bias for first hidden layer
    b2 = np.random.randn(hidden_layer_size_2)  # Bias for second hidden layer
    b3 = np.random.randn(output_size)  # Bias for output layer

    return W1, W2, W3, b1, b2, b3

# Function to perform forward pass
def forward_pass(X, W1, W2, W3, b1, b2, b3):
    Z1 = np.dot(X, W1) + b1  # Linear combination for first hidden layer
    A1 = tanh(Z1)  # Tanh activation

    Z2 = np.dot(A1, W2) + b2  # Linear combination for second hidden layer
    A2 = tanh(Z2)  # Tanh activation

    Z3 = np.dot(A2, W3) + b3  # Linear combination for output layer
    A3 = tanh(Z3)  # Tanh activation (output layer)

    return A1, A2, A3, Z1, Z2, Z3

# Function to perform backpropagation and update weights
def backpropagate(X, Y, A1, A2, A3, Z1, Z2, Z3, W1, W2, W3, b1, b2, b3, learning_rate):
    m = X.shape[0]  # Number of training examples

    # Compute the error at the output layer
    dA3 = A3 - Y
    dZ3 = dA3 * tanh_derivative(A3)
    dW3 = np.dot(A2.T, dZ3) / m
    db3 = np.sum(dZ3, axis=0) / m

    # Compute the error at the second hidden layer
    dA2 = np.dot(dZ3, W3.T)
    dZ2 = dA2 * tanh_derivative(A2)
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0) / m

    # Compute the error at the first hidden layer
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * tanh_derivative(A1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0) / m

    # Update the weights and biases using the gradient descent
    W1 -= learning_rate * dW1
    W2 -= learning_rate * dW2
    W3 -= learning_rate * dW3
    b1 -= learning_rate * db1
    b2 -= learning_rate * db2
    b3 -= learning_rate * db3

    return W1, W2, W3, b1, b2, b3

# Function to train the MLP
def train_mlp(X, Y, input_size, hidden_layer_size_1, hidden_layer_size_2, output_size, epochs=1000, learning_rate=0.01):
    W1, W2, W3, b1, b2, b3 = initialize_weights_and_biases(input_size, hidden_layer_size_1, hidden_layer_size_2, output_size)

    for epoch in range(epochs):
        # Perform forward pass
        A1, A2, A3, Z1, Z2, Z3 = forward_pass(X, W1, W2, W3, b1, b2, b3)

        # Perform backpropagation and update weights and biases
        W1, W2, W3, b1, b2, b3 = backpropagate(X, Y, A1, A2, A3, Z1, Z2, Z3, W1, W2, W3, b1, b2, b3, learning_rate)

        # Display the loss (mean squared error) every 100 epochs
        if epoch % 100 == 0:
            loss = np.mean((A3 - Y) ** 2)
            print(f"Epoch {epoch}, Loss: {loss}")

    return W1, W2, W3, b1, b2, b3

# Example of usage
# Define the input size (N), hidden layer sizes, and output size
input_size = 4  # Number of binary inputs
hidden_layer_size_1 = 5  # Number of neurons in the first hidden layer
hidden_layer_size_2 = 3  # Number of neurons in the second hidden layer
output_size = 1  # Number of binary outputs (1 output)

# Create random binary input data (X) and binary output data (Y)
X = np.random.randint(0, 2, (10, input_size))  # 10 samples, 4 binary inputs each
Y = np.random.randint(0, 2, (10, output_size))  # 10 samples, 1 binary output each

# Train the MLP and get the final weights and biases
W1, W2, W3, b1, b2, b3 = train_mlp(X, Y, input_size, hidden_layer_size_1, hidden_layer_size_2, output_size, epochs=1000, learning_rate=0.01)

# Display final weight matrices and biases
print("\nFinal Weights and Biases after training:")
print("W1 (Input to First Hidden Layer):\n", W1)
print("W2 (First Hidden Layer to Second Hidden Layer):\n", W2)
print("W3 (Second Hidden Layer to Output Layer):\n", W3)
print("b1 (Bias for First Hidden Layer):\n", b1)
print("b2 (Bias for Second Hidden Layer):\n", b2)
print("b3 (Bias for Output Layer):\n", b3)

#Tambe
#17
#Tambe
import numpy as np
from itertools import product

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size1, hidden_layer_size2, output_size):
    W1 = np.random.randn(input_size, hidden_layer_size1)
    W2 = np.random.randn(hidden_layer_size1, hidden_layer_size2)
    W3 = np.random.randn(hidden_layer_size2, output_size)

    b1 = np.random.randn(hidden_layer_size1)
    b2 = np.random.randn(hidden_layer_size2)
    b3 = np.random.randn(output_size)

    return W1, W2, W3, b1, b2, b3

# Forward pass (no activation)
def forward_pass(X, W1, W2, W3, b1, b2, b3):
    Z1 = np.dot(X, W1) + b1
    Z2 = np.dot(Z1, W2) + b2
    Z3 = np.dot(Z2, W3) + b3
    return Z1, Z2, Z3

# Training
def train_mlp(input_values, expected_outputs, input_size, hidden_layer_size1, hidden_layer_size2, output_size, threshold, steps=10000):
    W1, W2, W3, b1, b2, b3 = initialize_weights_and_biases(input_size, hidden_layer_size1, hidden_layer_size2, output_size)

    learning_rate = 0.5
    iteration = 0

    while iteration < steps:
        iteration += 1
        all_matched = True

        for i in range(len(input_values)):
            X = input_values[i]
            Y = expected_outputs[i]

            Z1, Z2, Z3 = forward_pass(X, W1, W2, W3, b1, b2, b3)

            output = Z3
            thresholded_output = 1 if output >= threshold else 0

            if thresholded_output != Y:
                all_matched = False

                # Error
                error = Y - output

                # Backpropagate
                dZ3 = error
                dZ2 = np.dot(W3, dZ3)
                dZ1 = np.dot(W2, dZ2)

                W3 += learning_rate * np.outer(Z2, dZ3)
                W2 += learning_rate * np.outer(Z1, dZ2)
                W1 += learning_rate * np.outer(X, dZ1)

        if all_matched:
            break

    return W1, W2, W3, b1, b2, b3, iteration

# Input
n = int(input("Enter number of binary inputs (N): "))
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter the corresponding outputs:")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

# Random threshold
threshold = round(np.random.uniform(0.1, 1.0), 1)
print("\nRandom Threshold Selected:", threshold)

# Train
W1, W2, W3, b1, b2, b3, steps = train_mlp(
    input_values=input_values,
    expected_outputs=expected_outputs,
    input_size=n,
    hidden_layer_size1=n,
    hidden_layer_size2=n,
    output_size=1,
    threshold=threshold
)

print("\nTraining Completed in", steps, "iterations")
print("\nFinal Weights and Biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("b1:", b1)
print("b2:", b2)
print("b3:", b3)
print("Threshold:", threshold)

print("\nResults:")
for i in range(len(input_values)):
    Z1, Z2, Z3 = forward_pass(input_values[i], W1, W2, W3, b1, b2, b3)
    output = Z3
    thresholded_output = 1 if output >= threshold else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#Tambe
import numpy as np
from itertools import product

def initialize_weights_and_biases(input_size, hidden_layer_size, output_size):
    W1 = np.random.randn(input_size, hidden_layer_size)
    W2 = np.random.randn(hidden_layer_size, output_size)
    b1 = np.random.randn(hidden_layer_size)
    b2 = np.random.randn(output_size)
    return W1, W2, b1, b2

def forward_pass(X, W1, W2, b1, b2):
    Z1 = np.dot(X, W1) + b1
    Z2 = np.dot(Z1, W2) + b2
    return Z1, Z2

def train_mlp(input_values, expected_outputs, input_size, hidden_layer_size, output_size, threshold, steps=10000):
    W1, W2, b1, b2 = initialize_weights_and_biases(input_size, hidden_layer_size, output_size)

    learning_rate = 0.5
    iteration = 0

    while iteration < steps:
        iteration += 1
        all_matched = True

        for i in range(len(input_values)):
            X = input_values[i]
            Y = expected_outputs[i]

            Z1, Z2 = forward_pass(X, W1, W2, b1, b2)
            output = Z2
            thresholded_output = (output >= threshold).astype(int)

            if not np.array_equal(thresholded_output, Y):
                all_matched = False
                error = Y - output

                dZ2 = error
                dZ1 = np.dot(W2, dZ2.T)

                W2 += learning_rate * np.outer(Z1, dZ2)
                W1 += learning_rate * np.outer(X, dZ1)

        if all_matched:
            break

    return W1, W2, b1, b2, iteration

# Input
n = 4
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter two outputs for each input:")
expected_outputs = []
for inp in input_values:
    o1 = int(input(f"{inp} - Output 1: "))
    o2 = int(input(f"{inp} - Output 2: "))
    expected_outputs.append([o1, o2])
expected_outputs = np.array(expected_outputs)

threshold = round(np.random.uniform(0.1, 1.0), 1)
print("\nRandom Threshold Selected:", threshold)

# Train
W1, W2, b1, b2, steps = train_mlp(
    input_values=input_values,
    expected_outputs=expected_outputs,
    input_size=4,
    hidden_layer_size=4,
    output_size=2,
    threshold=threshold
)

print("\nTraining Completed in", steps, "iterations")
print("\nFinal Weights and Biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("b1:", b1)
print("b2:", b2)
print("Threshold:", threshold)

print("\nResults:")
for i in range(len(input_values)):
    Z1, Z2 = forward_pass(input_values[i], W1, W2, b1, b2)
    output = Z2
    thresholded_output = (output >= threshold).astype(int)

    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i]}")
    print(f"Output of NN: {output}")
    print(f"Thresholded Output: {thresholded_output}")