# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWU-VzdS08eRNks75dbVN8fQdrWR3KeH
"""

# Assignment 17 - Educational Style - No Activation - No Functions

import numpy as np
from itertools import product

# Inputs
n = int(input("Enter number of binary inputs (N): "))
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter the corresponding outputs:")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

# Random initialization
np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])
threshold = round(np.random.uniform(0.1, 1.0), 1)

print("\nInitial weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("Threshold:", threshold)

learning_rate = 0.5
max_iterations = 10000
iteration = 0

while iteration < max_iterations:
    iteration += 1
    all_matched = True

    for i in range(len(input_values)):
        x = input_values[i]
        y = expected_outputs[i]

        # Forward pass manually (NO activation function)
        h1_input = np.dot(x, W1) + b1
        h1_output = h1_input

        h2_input = np.dot(h1_output, W2) + b2
        h2_output = h2_input

        output_input = np.dot(h2_output, W3) + b3
        output = output_input

        # Thresholding
        thresholded_output = 1 if output >= threshold else 0

        if thresholded_output != y:
            all_matched = False
            error = y - output

            # Backpropagation manually
            d_output = error
            d_h2 = np.dot(W3, d_output)
            d_h1 = np.dot(W2, d_h2)

            # Update weights manually
            W3 += learning_rate * np.outer(h2_output, d_output)
            W2 += learning_rate * np.outer(h1_output, d_h2)
            W1 += learning_rate * np.outer(x, d_h1)

    if all_matched:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nFinal weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("Threshold:", threshold)

# Final results
print("\nResults:")
for i in range(len(input_values)):
    x = input_values[i]
    y = expected_outputs[i]

    h1_input = np.dot(x, W1) + b1
    h1_output = h1_input
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = h2_input
    output_input = np.dot(h2_output, W3) + b3
    output = output_input

    thresholded_output = 1 if output >= threshold else 0

    print(f"\nInput: {x}")
    print(f"Expected Output: {y[0]}")
    print(f"Network Output: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

# Assignment 18 - Educational Style - No Activation - No Functions

import numpy as np
from itertools import product

# Inputs
n = 4  # Fixed 4 inputs
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter two outputs for each input:")
expected_outputs = []
for inp in input_values:
    o1 = int(input(f"{inp} - Output 1: "))
    o2 = int(input(f"{inp} - Output 2: "))
    expected_outputs.append([o1, o2])
expected_outputs = np.array(expected_outputs)

# Random initialization
np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, 2))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
threshold = round(np.random.uniform(0.1, 1.0), 1)

print("\nInitial weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Threshold:", threshold)

learning_rate = 0.5
max_iterations = 10000
iteration = 0

while iteration < max_iterations:
    iteration += 1
    all_matched = True

    for i in range(len(input_values)):
        x = input_values[i]
        y = expected_outputs[i]

        # Forward pass manually (NO activation function)
        h1_input = np.dot(x, W1) + b1
        h1_output = h1_input

        output_input = np.dot(h1_output, W2) + b2
        output = output_input

        # Thresholding
        thresholded_output = (output >= threshold).astype(int)

        if not np.array_equal(thresholded_output, y):
            all_matched = False
            error = y - output

            # Backpropagation manually
            d_output = error
            d_h1 = np.dot(W2, d_output.T)

            # Update weights manually
            W2 += learning_rate * np.outer(h1_output, d_output)
            W1 += learning_rate * np.outer(x, d_h1)

    if all_matched:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nFinal weights and biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Threshold:", threshold)

# Final results
print("\nResults:")
for i in range(len(input_values)):
    x = input_values[i]
    y = expected_outputs[i]

    h1_input = np.dot(x, W1) + b1
    h1_output = h1_input
    output_input = np.dot(h1_output, W2) + b2
    output = output_input

    thresholded_output = (output >= threshold).astype(int)

    print(f"\nInput: {x}")
    print(f"Expected Output: {y}")
    print(f"Network Output: {output}")
    print(f"Thresholded Output: {thresholded_output}")

import numpy as np

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size, output_size):
    # Initialize weights with random values
    W1 = np.random.randn(input_size, hidden_layer_size)  # Weights for input to hidden layer
    W2 = np.random.randn(hidden_layer_size, output_size)  # Weights for hidden layer to output layer

    # Initialize biases with random values
    b1 = np.random.randn(hidden_layer_size)  # Bias for hidden layer
    b2 = np.random.randn(output_size)  # Bias for output layer

    return W1, W2, b1, b2

# Function to perform forward pass (no activation function)
def forward_pass(X, W1, W2, b1, b2):
    # First hidden layer
    Z1 = np.dot(X, W1) + b1
    # Output layer
    Z2 = np.dot(Z1, W2) + b2
    return Z1, Z2

# Function to perform training (using random input and output data)
def train_mlp(input_size, hidden_layer_size, output_size, steps=1000):
    # Initialize random weights and biases
    W1, W2, b1, b2 = initialize_weights_and_biases(input_size, hidden_layer_size, output_size)

    # Random binary inputs and outputs (for simplicity)
    X = np.random.randint(0, 2, (1, input_size))  # 1 sample with 4 binary inputs
    Y = np.random.randint(0, 2, (1, output_size))  # 2 binary outputs

    for step in range(steps):
        # Forward pass
        Z1, Z2 = forward_pass(X, W1, W2, b1, b2)

        # Displaying intermediate results every 100 steps
        if step % 100 == 0:
            print(f"Step {step}:")
            print(f"Input (X): {X}")
            print(f"Output (Y): {Y}")
            print(f"Hidden Layer Output (Z1): {Z1}")
            print(f"Output Layer (Z2): {Z2}")
            print(f"Weights (W1): {W1}")
            print(f"Weights (W2): {W2}")
            print(f"Biases (b1): {b1}")
            print(f"Biases (b2): {b2}")
            print()

    # Return final weight matrices, biases, and steps
    return W1, W2, b1, b2, steps

# Example of usage
input_size = 4  # Number of binary inputs
hidden_layer_size = 5  # Size of hidden layer
output_size = 2  # Number of binary outputs

# Train the MLP and display the results
W1, W2, b1, b2, steps = train_mlp(input_size, hidden_layer_size, output_size, steps=1000)

print(f"Final Weights (W1):\n{W1}")
print(f"Final Weights (W2):\n{W2}")
print(f"Final Biases (b1):\n{b1}")
print(f"Final Biases (b2):\n{b2}")
print(f"Total Steps: {steps}")

import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 10000
iteration = 0

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = sigmoid(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = sigmoid(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = sigmoid(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * sigmoid_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * sigmoid_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * sigmoid_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = sigmoid(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = sigmoid(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = sigmoid(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 10000
iteration = 0

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = sigmoid(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = sigmoid(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = sigmoid(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * sigmoid_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * sigmoid_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * sigmoid_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = sigmoid(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = sigmoid(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = sigmoid(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

import numpy as np
from itertools import product

n = int(input("How many binary inputs are there? "))
input_values = np.array(list(product([0, 1], repeat=n)))
print("\nFollowing are the combinations of inputs. Enter the corresponding outputs")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

np.random.seed(42)
W1 = np.random.uniform(-1, 1, (n, n))
W2 = np.random.uniform(-1, 1, (n, n))
W3 = np.random.uniform(-1, 1, (n, 1))
b1 = np.random.choice([-1, 0, 1])
b2 = np.random.choice([-1, 0, 1])
b3 = np.random.choice([-1, 0, 1])

print("\nTraining step 1")
print("Bias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

learning_rate = 0.5
max_iterations = 10000
iteration = 0

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x) ** 2

while True:
    iteration += 1
    all_matched = True
    for i in range(len(input_values)):
        # Forward
        h1_input = np.dot(input_values[i], W1) + b1
        h1_output = sigmoid(h1_input)
        h2_input = np.dot(h1_output, W2) + b2
        h2_output = sigmoid(h2_input)
        output_input = np.dot(h2_output, W3) + b3
        output = sigmoid(output_input)

        thresholded_output = 1 if output >= 0.5 else 0

        if thresholded_output != expected_outputs[i][0]:
            all_matched = False
            # Backpropagation
            error = expected_outputs[i] - output
            d_output = error * sigmoid_derivative(output_input)
            d_h2 = np.dot(W3, d_output) * sigmoid_derivative(h2_input)
            d_h1 = np.dot(W2, d_h2) * sigmoid_derivative(h1_input)

            W3 += learning_rate * np.dot(h2_output.reshape(-1, 1), d_output.reshape(1, -1))
            W2 += learning_rate * np.dot(h1_output.reshape(-1, 1), d_h2.reshape(1, -1))
            W1 += learning_rate * np.dot(input_values[i].reshape(n, 1), d_h1.reshape(1, -1))

    if all_matched or iteration >= max_iterations:
        break

print("\nTraining completed in", iteration, "iterations")
print("\nBias 1:", b1)
print("Bias 2:", b2)
print("Bias 3:", b3)
print("\nWeight 1 matrix\n", W1)
print("\nWeight 2 matrix\n", W2)
print("\nWeight 3 matrix\n", W3.flatten())

# Results
print("\nResults:")
for i in range(len(input_values)):
    h1_input = np.dot(input_values[i], W1) + b1
    h1_output = sigmoid(h1_input)
    h2_input = np.dot(h1_output, W2) + b2
    h2_output = sigmoid(h2_input)
    output_input = np.dot(h2_output, W3) + b3
    output = sigmoid(output_input)
    thresholded_output = 1 if output >= 0.5 else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#Tambe
#17
#Tambe
import numpy as np
from itertools import product

# Function to initialize random weights and biases
def initialize_weights_and_biases(input_size, hidden_layer_size1, hidden_layer_size2, output_size):
    W1 = np.random.randn(input_size, hidden_layer_size1)
    W2 = np.random.randn(hidden_layer_size1, hidden_layer_size2)
    W3 = np.random.randn(hidden_layer_size2, output_size)

    b1 = np.random.randn(hidden_layer_size1)
    b2 = np.random.randn(hidden_layer_size2)
    b3 = np.random.randn(output_size)

    return W1, W2, W3, b1, b2, b3

# Forward pass (no activation)
def forward_pass(X, W1, W2, W3, b1, b2, b3):
    Z1 = np.dot(X, W1) + b1
    Z2 = np.dot(Z1, W2) + b2
    Z3 = np.dot(Z2, W3) + b3
    return Z1, Z2, Z3

# Training
def train_mlp(input_values, expected_outputs, input_size, hidden_layer_size1, hidden_layer_size2, output_size, threshold, steps=10000):
    W1, W2, W3, b1, b2, b3 = initialize_weights_and_biases(input_size, hidden_layer_size1, hidden_layer_size2, output_size)

    learning_rate = 0.5
    iteration = 0

    while iteration < steps:
        iteration += 1
        all_matched = True

        for i in range(len(input_values)):
            X = input_values[i]
            Y = expected_outputs[i]

            Z1, Z2, Z3 = forward_pass(X, W1, W2, W3, b1, b2, b3)

            output = Z3
            thresholded_output = 1 if output >= threshold else 0

            if thresholded_output != Y:
                all_matched = False

                # Error
                error = Y - output

                # Backpropagate
                dZ3 = error
                dZ2 = np.dot(W3, dZ3)
                dZ1 = np.dot(W2, dZ2)

                W3 += learning_rate * np.outer(Z2, dZ3)
                W2 += learning_rate * np.outer(Z1, dZ2)
                W1 += learning_rate * np.outer(X, dZ1)

        if all_matched:
            break

    return W1, W2, W3, b1, b2, b3, iteration

# Input
n = int(input("Enter number of binary inputs (N): "))
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter the corresponding outputs:")
expected_outputs = [int(input(f"{inp} - ")) for inp in input_values]
expected_outputs = np.array(expected_outputs).reshape(-1, 1)

# Random threshold
threshold = round(np.random.uniform(0.1, 1.0), 1)
print("\nRandom Threshold Selected:", threshold)

# Train
W1, W2, W3, b1, b2, b3, steps = train_mlp(
    input_values=input_values,
    expected_outputs=expected_outputs,
    input_size=n,
    hidden_layer_size1=n,
    hidden_layer_size2=n,
    output_size=1,
    threshold=threshold
)

print("\nTraining Completed in", steps, "iterations")
print("\nFinal Weights and Biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("W3:\n", W3.flatten())
print("b1:", b1)
print("b2:", b2)
print("b3:", b3)
print("Threshold:", threshold)

print("\nResults:")
for i in range(len(input_values)):
    Z1, Z2, Z3 = forward_pass(input_values[i], W1, W2, W3, b1, b2, b3)
    output = Z3
    thresholded_output = 1 if output >= threshold else 0
    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i][0]}")
    print(f"Output of NN: {output.item():.4f}")
    print(f"Thresholded Output: {thresholded_output}")

#Tambe
import numpy as np
from itertools import product

def initialize_weights_and_biases(input_size, hidden_layer_size, output_size):
    W1 = np.random.randn(input_size, hidden_layer_size)
    W2 = np.random.randn(hidden_layer_size, output_size)
    b1 = np.random.randn(hidden_layer_size)
    b2 = np.random.randn(output_size)
    return W1, W2, b1, b2

def forward_pass(X, W1, W2, b1, b2):
    Z1 = np.dot(X, W1) + b1
    Z2 = np.dot(Z1, W2) + b2
    return Z1, Z2

def train_mlp(input_values, expected_outputs, input_size, hidden_layer_size, output_size, threshold, steps=10000):
    W1, W2, b1, b2 = initialize_weights_and_biases(input_size, hidden_layer_size, output_size)

    learning_rate = 0.5
    iteration = 0

    while iteration < steps:
        iteration += 1
        all_matched = True

        for i in range(len(input_values)):
            X = input_values[i]
            Y = expected_outputs[i]

            Z1, Z2 = forward_pass(X, W1, W2, b1, b2)
            output = Z2
            thresholded_output = (output >= threshold).astype(int)

            if not np.array_equal(thresholded_output, Y):
                all_matched = False
                error = Y - output

                dZ2 = error
                dZ1 = np.dot(W2, dZ2.T)

                W2 += learning_rate * np.outer(Z1, dZ2)
                W1 += learning_rate * np.outer(X, dZ1)

        if all_matched:
            break

    return W1, W2, b1, b2, iteration

# Input
n = 4
input_values = np.array(list(product([0, 1], repeat=n)))

print("\nFollowing are the combinations of inputs. Enter two outputs for each input:")
expected_outputs = []
for inp in input_values:
    o1 = int(input(f"{inp} - Output 1: "))
    o2 = int(input(f"{inp} - Output 2: "))
    expected_outputs.append([o1, o2])
expected_outputs = np.array(expected_outputs)

threshold = round(np.random.uniform(0.1, 1.0), 1)
print("\nRandom Threshold Selected:", threshold)

# Train
W1, W2, b1, b2, steps = train_mlp(
    input_values=input_values,
    expected_outputs=expected_outputs,
    input_size=4,
    hidden_layer_size=4,
    output_size=2,
    threshold=threshold
)

print("\nTraining Completed in", steps, "iterations")
print("\nFinal Weights and Biases:")
print("W1:\n", W1)
print("W2:\n", W2)
print("b1:", b1)
print("b2:", b2)
print("Threshold:", threshold)

print("\nResults:")
for i in range(len(input_values)):
    Z1, Z2 = forward_pass(input_values[i], W1, W2, b1, b2)
    output = Z2
    thresholded_output = (output >= threshold).astype(int)

    print(f"\nInput: {input_values[i]}")
    print(f"Expected Output: {expected_outputs[i]}")
    print(f"Output of NN: {output}")
    print(f"Thresholded Output: {thresholded_output}")