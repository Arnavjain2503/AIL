{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nE05SfhYDkU",
        "outputId": "f374c4c1-d5c2-48d7-a3a1-a1f9f95c9309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Full Program\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read text\n",
        "with open('textfile_22.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# a. Text cleaning (remove punctuation, special characters, numbers)\n",
        "text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# b. Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# c. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# d. Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "# e. Correct misspelled words\n",
        "corrected_tokens = []\n",
        "for word in filtered_tokens:\n",
        "    corrected_word = str(TextBlob(word).correct())\n",
        "    corrected_tokens.append(corrected_word)\n",
        "\n",
        "# Display final output\n",
        "print(\"Corrected Tokens:\")\n",
        "print(corrected_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQJg3R-dvS9w",
        "outputId": "fb88852c-a3c3-4804-ae85-d359584213a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Tokens:\n",
            "['artificial', 'intelligence', 'ai', 'transforming', 'world', 'rapidly', 'used', 'various', 'fields', 'like', 'healthcare', 'finance', 'agriculture', 'education', 'machine', 'learning', 'my', 'sunset', 'ai', 'enables', 'computers', 'learn', 'data', 'natural', 'language', 'processing', 'nap', 'helps', 'machines', 'understand', 'human', 'language', 'computer', 'vision', 'allows', 'systems', 'interpret', 'visual', 'information', 'like', 'images', 'video', 'robotics', 'another', 'exciting', 'branch', 'ai', 'helps', 'machines', 'perform', 'tasks', 'ai', 'also', 'enhancing', 'user', 'experiences', 'mobile', 'applications', 'voice', 'assistants', 'like', 'area', 'sir', 'work', 'based', 'ai', 'algorithms', 'autonomous', 'cars', 'possible', 'driven', 'decisionmaking', 'systems', 'cybersecurity', 'becoming', 'smarter', 'based', 'intrusion', 'detection', 'ai', 'used', 'fraud', 'detection', 'banking', 'systems', 'personalized', 'recommendations', 'netflix', 'amazon', 'lowered', 'ai', 'smart', 'farming', 'technique', 'adopted', 'using', 'ai', 'ai', 'also', 'helping', 'doctors', 'diagnose', 'diseases', 'accurately', 'empowered', 'whatnots', 'provide', 'quick', 'customer', 'service', 'deep', 'learning', 'part', 'ai', 'uses', 'neutral', 'network', 'neutral', 'network', 'mimi', 'human', 'brains', 'structure', 'ai', 'systems', 'trained', 'large', 'datasets', 'future', 'ai', 'looks', 'promising', 'expected', 'create', 'many', 'new', 'job', 'opportunities', 'ai', 'ethics', 'important', 'topic', 'discussion', 'bias', 'ai', 'models', 'must', 'addressed', 'explainable', 'ai', 'ensures', 'transparent', 'reinforcement', 'learning', 'helping', 'machines', 'learn', 'actions', 'generative', 'ai', 'create', 'new', 'content', 'like', 'images', 'music', 'text', 'ai', 'models', 'becoming', 'efficient', 'time', 'cloud', 'computing', 'helps', 'scale', 'ai', 'applications', 'ai', 'hardware', 'accelerators', 'like', 'thus', 'improve', 'performance', 'edge', 'ai', 'brings', 'intelligence', 'closer', 'devices', 'driven', 'analysis', 'help', 'business', 'make', 'better', 'decisions']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Full Program\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read text\n",
        "with open('textfile_23.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# a. Text cleaning\n",
        "text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# b. Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# c. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "# d. Create list of 3 consecutive words\n",
        "trigrams = []\n",
        "for i in range(len(lemmatized_tokens) - 2):\n",
        "    trigram = ' '.join(lemmatized_tokens[i:i+3])\n",
        "    trigrams.append(trigram)\n",
        "\n",
        "# Display trigrams\n",
        "print(\"Trigrams after Lemmatization:\")\n",
        "print(trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWk4qZK3oAHd",
        "outputId": "657bc802-b6fe-4ba7-d20e-eed8661ce597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigrams after Lemmatization:\n",
            "['artifici intellig ai', 'intellig ai is', 'ai is transform', 'is transform the', 'transform the world', 'the world rapidli', 'world rapidli it', 'rapidli it is', 'it is use', 'is use in', 'use in variou', 'in variou field', 'variou field like', 'field like healthcar', 'like healthcar financ', 'healthcar financ agricultur', 'financ agricultur and', 'agricultur and educ', 'and educ machin', 'educ machin learn', 'machin learn ml', 'learn ml is', 'ml is a', 'is a subset', 'a subset of', 'subset of ai', 'of ai that', 'ai that enabl', 'that enabl comput', 'enabl comput to', 'comput to learn', 'to learn from', 'learn from data', 'from data natur', 'data natur languag', 'natur languag process', 'languag process nlp', 'process nlp help', 'nlp help machin', 'help machin understand', 'machin understand human', 'understand human languag', 'human languag comput', 'languag comput vision', 'comput vision allow', 'vision allow system', 'allow system to', 'system to interpret', 'to interpret visual', 'interpret visual inform', 'visual inform like', 'inform like imag', 'like imag and', 'imag and video', 'and video robot', 'video robot is', 'robot is anoth', 'is anoth excit', 'anoth excit branch', 'excit branch where', 'branch where ai', 'where ai help', 'ai help machin', 'help machin to', 'machin to perform', 'to perform task', 'perform task ai', 'task ai is', 'ai is also', 'is also enhanc', 'also enhanc user', 'enhanc user experi', 'user experi in', 'experi in mobil', 'in mobil applic', 'mobil applic voic', 'applic voic assist', 'voic assist like', 'assist like alexa', 'like alexa and', 'alexa and siri', 'and siri work', 'siri work base', 'work base on', 'base on ai', 'on ai algorithm', 'ai algorithm autonom', 'algorithm autonom car', 'autonom car are', 'car are possibl', 'are possibl becaus', 'possibl becaus of', 'becaus of aidriven', 'of aidriven decisionmak', 'aidriven decisionmak system', 'decisionmak system cybersecur', 'system cybersecur is', 'cybersecur is becom', 'is becom smarter', 'becom smarter with', 'smarter with aibas', 'with aibas intrus', 'aibas intrus detect', 'intrus detect ai', 'detect ai is', 'ai is use', 'is use in', 'use in fraud', 'in fraud detect', 'fraud detect in', 'detect in bank', 'in bank system', 'bank system person', 'system person recommend', 'person recommend on', 'recommend on netflix', 'on netflix and', 'netflix and amazon', 'and amazon are', 'amazon are power', 'are power by', 'power by ai', 'by ai smart', 'ai smart farm', 'smart farm techniqu', 'farm techniqu are', 'techniqu are be', 'are be adopt', 'be adopt use', 'adopt use ai', 'use ai ai', 'ai ai is', 'ai is also', 'is also help', 'also help doctor', 'help doctor diagnos', 'doctor diagnos diseas', 'diagnos diseas more', 'diseas more accur', 'more accur aipow', 'accur aipow chatbot', 'aipow chatbot provid', 'chatbot provid quick', 'provid quick custom', 'quick custom servic', 'custom servic deep', 'servic deep learn', 'deep learn is', 'learn is a', 'is a part', 'a part of', 'part of ai', 'of ai that', 'ai that use', 'that use neural', 'use neural network', 'neural network neural', 'network neural network', 'neural network mimic', 'network mimic the', 'mimic the human', 'the human brain', 'human brain structur', 'brain structur ai', 'structur ai system', 'ai system are', 'system are be', 'are be train', 'be train with', 'train with larg', 'with larg dataset', 'larg dataset the', 'dataset the futur', 'the futur of', 'futur of ai', 'of ai look', 'ai look promis', 'look promis it', 'promis it is', 'it is expect', 'is expect to', 'expect to creat', 'to creat mani', 'creat mani new', 'mani new job', 'new job opportun', 'job opportun ai', 'opportun ai ethic', 'ai ethic is', 'ethic is an', 'is an import', 'an import topic', 'import topic of', 'topic of discus', 'of discus bia', 'discus bia in', 'bia in ai', 'in ai model', 'ai model must', 'model must be', 'must be address', 'be address explain', 'address explain ai', 'explain ai ensur', 'ai ensur transpar', 'ensur transpar reinforc', 'transpar reinforc learn', 'reinforc learn is', 'learn is help', 'is help machin', 'help machin learn', 'machin learn from', 'learn from action', 'from action gener', 'action gener ai', 'gener ai can', 'ai can creat', 'can creat new', 'creat new content', 'new content like', 'content like imag', 'like imag music', 'imag music and', 'music and text', 'and text ai', 'text ai model', 'ai model are', 'model are becom', 'are becom more', 'becom more effici', 'more effici with', 'effici with time', 'with time cloud', 'time cloud comput', 'cloud comput help', 'comput help scale', 'help scale ai', 'scale ai applic', 'ai applic ai', 'applic ai hardwar', 'ai hardwar acceler', 'hardwar acceler like', 'acceler like tpu', 'like tpu improv', 'tpu improv perform', 'improv perform edg', 'perform edg ai', 'edg ai bring', 'ai bring intellig', 'bring intellig closer', 'intellig closer to', 'closer to devic', 'to devic aidriven', 'devic aidriven analyt', 'aidriven analyt help', 'analyt help busi', 'help busi make', 'busi make better', 'make better decis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Full Program\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Read files\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Merge and split text into words\n",
        "all_text = ' '.join(texts)\n",
        "words = list(set(all_text.split()))\n",
        "\n",
        "# Reshape for sklearn\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "\n",
        "# One Hot Encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "onehot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# Display\n",
        "print(\"Words:\")\n",
        "print(words)\n",
        "print(\"\\nOne Hot Encoded Matrix:\")\n",
        "print(onehot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z32052_zpHnx",
        "outputId": "65da9e13-3e97-49ca-ef03-dbde04704cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words:\n",
            "['machines', 'diagnosis,', 'networks', 'AI.', 'chatbots,', 'world', 'recognition', 'is', 'multiple', 'of', 'processing.', 'major', 'Deep', 'speech', 'enables', 'Learning', 'Computer', 'Artificial', 'image', 'patterns.', 'revolutionized', 'Vision', 'Processing', 'to', 'surveillance.', 'a', 'layers', 'learn', 'understand,', 'medical', 'has', 'Language', 'interpret', 'human', 'ability', 'cars,', 'and', 'part', 'Intelligence.', 'visually.', 'self-driving', 'interpret,', 'in', 'neural', 'uses', 'see', 'sentiment', 'Natural', 'the', 'translation,', 'It', 'generate', 'field', 'applications', 'languages.', 'complex', 'analysis.', 'include', 'Applications']\n",
            "\n",
            "One Hot Encoded Matrix:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the 3 text files (Each file contains 20+ sentences, 150+ words)\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Split all text into words and get unique words (vocabulary)\n",
        "all_text = ' '.join(texts)\n",
        "words = list(set(all_text.split()))  # Unique words from the combined text\n",
        "\n",
        "# Create a mapping of word to column index\n",
        "word_to_index = {word: index for index, word in enumerate(words)}\n",
        "\n",
        "# Create a 2D array to hold the one-hot encoding matrix for all documents\n",
        "onehot_encoded = []\n",
        "\n",
        "# Loop through each document (text file)\n",
        "for document in texts:\n",
        "    # Initialize a list for this document\n",
        "    onehot_vector = [0] * len(words)\n",
        "\n",
        "    # Split the document into words and set the corresponding positions to 1\n",
        "    for word in document.split():\n",
        "        if word in word_to_index:  # Ensure the word exists in the vocabulary\n",
        "            onehot_vector[word_to_index[word]] = 1\n",
        "\n",
        "    # Append the one-hot vector of this document to the matrix\n",
        "    onehot_encoded.append(onehot_vector)\n",
        "\n",
        "# Convert the result into a NumPy array for easier handling\n",
        "onehot_encoded = np.array(onehot_encoded)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "onehot_df = pd.DataFrame(onehot_encoded, columns=words)\n",
        "\n",
        "# Display the vocabulary and the one-hot encoded matrix nicely\n",
        "print(\"\\nOne-Hot Encoded Matrix:\")\n",
        "print(onehot_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBQMHpaPudoO",
        "outputId": "82a536b2-5908-4ccf-ae6e-7d3caa7a914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words:\n",
            "['machines', 'diagnosis,', 'networks', 'AI.', 'chatbots,', 'world', 'recognition', 'is', 'multiple', 'of', 'processing.', 'major', 'Deep', 'speech', 'enables', 'Learning', 'Computer', 'Artificial', 'image', 'patterns.', 'revolutionized', 'Vision', 'Processing', 'to', 'surveillance.', 'a', 'layers', 'learn', 'understand,', 'medical', 'has', 'Language', 'interpret', 'human', 'ability', 'cars,', 'and', 'part', 'Intelligence.', 'visually.', 'self-driving', 'interpret,', 'in', 'neural', 'uses', 'see', 'sentiment', 'Natural', 'the', 'translation,', 'It', 'generate', 'field', 'applications', 'languages.', 'complex', 'analysis.', 'include', 'Applications']\n",
            "\n",
            "One-Hot Encoded Matrix:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the 3 text files (Each file contains 20+ sentences, 150+ words)\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Merge and split text into words\n",
        "all_text = ' '.join(texts)\n",
        "words = list(set(all_text.split()))  # Get unique words (vocabulary)\n",
        "\n",
        "# Initialize the OneHotEncoder (for document encoding)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Split the documents into words (list of words for each document)\n",
        "documents = [document.split() for document in texts]\n",
        "\n",
        "# Create a list where each document is represented as a list of words\n",
        "documents_vectorized = []\n",
        "\n",
        "# For each document, create a list of word occurrences as one-hot encoding\n",
        "for document in documents:\n",
        "    # Create a binary vector where 1 means the word exists in the document, 0 means it does not\n",
        "    one_hot_vector = [1 if word in document else 0 for word in words]\n",
        "    documents_vectorized.append(one_hot_vector)\n",
        "\n",
        "# Convert the result into a NumPy array for easier handling\n",
        "onehot_encoded_documents = np.array(documents_vectorized)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "onehot_df = pd.DataFrame(onehot_encoded_documents, columns=words)\n",
        "\n",
        "# Display the vocabulary and the one-hot encoded matrix nicely\n",
        "print(\"Vocabulary (Words):\")\n",
        "print(\"  \" + \"  \".join(words))  # Print the vocabulary words horizontally\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Matrix (Documents vs Words):\")\n",
        "print(onehot_df)  # Display the one-hot encoded DataFrame\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYvEKnPV3az8",
        "outputId": "4e3a19a0-39d3-4a7f-c501-1e78915561d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Words):\n",
            "  machines  diagnosis,  networks  AI.  chatbots,  world  recognition  is  multiple  of  processing.  major  Deep  speech  enables  Learning  Computer  Artificial  image  patterns.  revolutionized  Vision  Processing  to  surveillance.  a  layers  learn  understand,  medical  has  Language  interpret  human  ability  cars,  and  part  Intelligence.  visually.  self-driving  interpret,  in  neural  uses  see  sentiment  Natural  the  translation,  It  generate  field  applications  languages.  complex  analysis.  include  Applications\n",
            "\n",
            "One-Hot Encoded Matrix (Documents vs Words):\n",
            "   machines  diagnosis,  networks  AI.  chatbots,  world  recognition  is  \\\n",
            "0         0           0         1    0          0      0            1   1   \n",
            "1         1           0         0    1          1      0            0   1   \n",
            "2         1           1         0    0          0      1            0   1   \n",
            "\n",
            "   multiple  of  ...  translation,  It  generate  field  applications  \\\n",
            "0         1   1  ...             0   1         0      0             0   \n",
            "1         0   0  ...             1   1         1      1             0   \n",
            "2         0   1  ...             0   1         0      0             1   \n",
            "\n",
            "   languages.  complex  analysis.  include  Applications  \n",
            "0           0        1          0        0             0  \n",
            "1           1        0          1        1             1  \n",
            "2           0        0          0        0             0  \n",
            "\n",
            "[3 rows x 59 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the 3 text files (Each file contains 20+ sentences, 150+ words)\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Split all text into words and get unique words (vocabulary)\n",
        "all_text = ' '.join(texts)\n",
        "words = list(set(all_text.split()))  # Unique words from the combined text\n",
        "\n",
        "# Create a mapping of word to column index\n",
        "word_to_index = {word: index for index, word in enumerate(words)}\n",
        "\n",
        "# Create a 2D array to hold the one-hot encoding matrix for all documents\n",
        "onehot_encoded = []\n",
        "\n",
        "# Loop through each document (text file)\n",
        "for document in texts:\n",
        "    # Initialize a list for this document\n",
        "    onehot_vector = [0] * len(words)\n",
        "\n",
        "    # Split the document into words and set the corresponding positions to 1\n",
        "    for word in document.split():\n",
        "        if word in word_to_index:  # Ensure the word exists in the vocabulary\n",
        "            onehot_vector[word_to_index[word]] = 1\n",
        "\n",
        "    # Append the one-hot vector of this document to the matrix\n",
        "    onehot_encoded.append(onehot_vector)\n",
        "\n",
        "# Convert the result into a NumPy array for easier handling\n",
        "onehot_encoded = np.array(onehot_encoded)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "onehot_df = pd.DataFrame(onehot_encoded, columns=words)\n",
        "\n",
        "# Display the vocabulary and the one-hot encoded matrix nicely\n",
        "print(\"Vocabulary:\")\n",
        "print(\"  \" + \"  \".join(words))  # Print the vocabulary words horizontally\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Matrix:\")\n",
        "\n",
        "# Loop through each document to print its one-hot encoded values\n",
        "for i, onehot_vector in enumerate(onehot_encoded):\n",
        "    print(f\"Document {i+1}: \", end=\"\")\n",
        "    for value in onehot_vector:\n",
        "        print(f\"{value}\", end=\"   \")\n",
        "    print()  # Newline after each document's encoding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpPJasIzhzy",
        "outputId": "83ddf799-7482-4a90-c6b1-c8874f300417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "  machines  diagnosis,  networks  AI.  chatbots,  world  recognition  is  multiple  of  processing.  major  Deep  speech  enables  Learning  Computer  Artificial  image  patterns.  revolutionized  Vision  Processing  to  surveillance.  a  layers  learn  understand,  medical  has  Language  interpret  human  ability  cars,  and  part  Intelligence.  visually.  self-driving  interpret,  in  neural  uses  see  sentiment  Natural  the  translation,  It  generate  field  applications  languages.  complex  analysis.  include  Applications\n",
            "\n",
            "One-Hot Encoded Matrix:\n",
            "Document 1: 0   0   1   0   0   0   1   1   1   1   1   0   1   1   0   1   0   1   1   1   1   0   0   1   0   1   1   1   0   0   1   0   0   0   0   0   1   1   1   0   0   0   0   1   1   0   0   0   0   0   1   0   0   0   0   1   0   0   0   \n",
            "Document 2: 1   0   0   1   1   0   0   1   0   0   0   1   0   0   1   0   0   0   0   0   0   0   1   1   0   1   0   0   1   0   0   1   0   1   0   0   1   0   0   0   0   1   1   0   0   0   1   1   0   1   1   1   1   0   1   0   1   1   1   \n",
            "Document 3: 1   1   0   0   0   1   0   1   0   1   0   0   0   0   0   0   1   0   0   0   0   1   0   1   1   0   0   0   0   1   1   0   1   0   1   1   1   0   0   1   1   0   1   0   0   1   0   0   1   0   1   0   0   1   0   0   0   0   0   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "def one_hot_encode(text, vocabulary):\n",
        "    encoded_data = {}\n",
        "    for word in vocabulary:\n",
        "        encoded_data[word] = []\n",
        "\n",
        "    for word in text.split():\n",
        "        vector = [0] * len(vocabulary)\n",
        "        if word in vocabulary:\n",
        "            vector[vocabulary.index(word)] = 1\n",
        "        for i, vocab_word in enumerate(vocabulary):\n",
        "            encoded_data[vocab_word].append(vector[i])\n",
        "\n",
        "    df = pd.DataFrame(encoded_data, index=text.split())\n",
        "    return df\n",
        "\n",
        "file1 = open(\"textfile_24_1.txt\", \"r\").read()\n",
        "file2 = open(\"textfile_24_2.txt\", \"r\").read()\n",
        "file3 = open(\"textfile_24_3.txt\", \"r\").read()\n",
        "\n",
        "text = file1 + \" \" + file2 + \" \" + file3\n",
        "\n",
        "print(text)\n",
        "\n",
        "vocabulary = list(set(text.lower().split()))\n",
        "vocabulary.sort()\n",
        "\n",
        "\n",
        "encoded_df = one_hot_encode(text.lower(), vocabulary)\n",
        "\n",
        "print(encoded_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ClMr-FtQQdd",
        "outputId": "31b726c6-6511-452a-dccc-dc73f4d5259c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Learning is a part of Artificial Intelligence. It uses multiple layers of neural networks to learn complex patterns. It has revolutionized image recognition and speech processing.\n",
            " Natural Language Processing is a major field in AI. It enables machines to understand, interpret, and generate human languages. Applications include chatbots, translation, and sentiment analysis.\n",
            " Computer Vision is the ability of machines to see and interpret the world visually. It has applications in self-driving cars, medical diagnosis, and surveillance.\n",
            "\n",
            "               a  ability  ai.  analysis.  and  applications  artificial  \\\n",
            "deep           0        0    0          0    0             0           0   \n",
            "learning       0        0    0          0    0             0           0   \n",
            "is             0        0    0          0    0             0           0   \n",
            "a              1        0    0          0    0             0           0   \n",
            "part           0        0    0          0    0             0           0   \n",
            "...           ..      ...  ...        ...  ...           ...         ...   \n",
            "cars,          0        0    0          0    0             0           0   \n",
            "medical        0        0    0          0    0             0           0   \n",
            "diagnosis,     0        0    0          0    0             0           0   \n",
            "and            0        0    0          0    1             0           0   \n",
            "surveillance.  0        0    0          0    0             0           0   \n",
            "\n",
            "               cars,  chatbots,  complex  ...  speech  surveillance.  the  to  \\\n",
            "deep               0          0        0  ...       0              0    0   0   \n",
            "learning           0          0        0  ...       0              0    0   0   \n",
            "is                 0          0        0  ...       0              0    0   0   \n",
            "a                  0          0        0  ...       0              0    0   0   \n",
            "part               0          0        0  ...       0              0    0   0   \n",
            "...              ...        ...      ...  ...     ...            ...  ...  ..   \n",
            "cars,              1          0        0  ...       0              0    0   0   \n",
            "medical            0          0        0  ...       0              0    0   0   \n",
            "diagnosis,         0          0        0  ...       0              0    0   0   \n",
            "and                0          0        0  ...       0              0    0   0   \n",
            "surveillance.      0          0        0  ...       0              1    0   0   \n",
            "\n",
            "               translation,  understand,  uses  vision  visually.  world  \n",
            "deep                      0            0     0       0          0      0  \n",
            "learning                  0            0     0       0          0      0  \n",
            "is                        0            0     0       0          0      0  \n",
            "a                         0            0     0       0          0      0  \n",
            "part                      0            0     0       0          0      0  \n",
            "...                     ...          ...   ...     ...        ...    ...  \n",
            "cars,                     0            0     0       0          0      0  \n",
            "medical                   0            0     0       0          0      0  \n",
            "diagnosis,                0            0     0       0          0      0  \n",
            "and                       0            0     0       0          0      0  \n",
            "surveillance.             0            0     0       0          0      0  \n",
            "\n",
            "[77 rows x 58 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Read the 3 text files (Each file contains 20+ sentences, 150+ words)\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'textfile_25_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Convert the result to a DataFrame for better readability\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the vocabulary and the BoW matrix nicely\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "zJsQnFdxpJE8",
        "outputId": "907d685a-7842-4b1b-add8-6e5df10a798f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'textfile_25_1.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d3beb0f85a83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'textfile_25_{i}.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'textfile_25_1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Full Program\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Read files\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Display\n",
        "print(\"Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phloGb4SpKpu",
        "outputId": "76121654-773d-4cff-948c-ee3c3b04938b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['adds' 'advancements' 'agra' 'alleppey' 'also' 'amazing' 'amer' 'an'\n",
            " 'ancient' 'and' 'another' 'architecture' 'are' 'artisans' 'as' 'attract'\n",
            " 'attractions' 'ayurvedic' 'backwaters' 'bazaars' 'beaches' 'beauty'\n",
            " 'blend' 'breathtaking' 'circuit' 'city' 'coconut' 'colorful' 'country'\n",
            " 'craftsmanship' 'cuisine' 'culture' 'dance' 'deeply' 'delicious'\n",
            " 'destinations' 'elephants' 'era' 'essential' 'famous' 'fatehpur'\n",
            " 'festivals' 'filled' 'for' 'fort' 'gangaur' 'god' 'golden' 'handicrafts'\n",
            " 'hawa' 'heritage' 'highlight' 'history' 'home' 'hospitality' 'houseboat'\n",
            " 'impressive' 'in' 'including' 'india' 'is' 'its' 'jaipur' 'jantar'\n",
            " 'jewelry' 'kathakali' 'kerala' 'known' 'kumarakom' 'local' 'love' 'mahal'\n",
            " 'mantar' 'many' 'marble' 'markets' 'mesmerizing' 'modern' 'monuments'\n",
            " 'mughal' 'munnar' 'must' 'nature' 'of' 'offer' 'offers' 'one' 'own'\n",
            " 'palace' 'perfect' 'periyar' 'pink' 'plantations' 'popular' 'provide'\n",
            " 'renowned' 'rich' 'rides' 'river' 'rooted' 'royal' 'sanctuaries'\n",
            " 'sanctuary' 'scenic' 'seven' 'significant' 'sikri' 'sites' 'souvenirs'\n",
            " 'spices' 'stop' 'symbol' 'taj' 'tea' 'teej' 'the' 'therapies' 'tigers'\n",
            " 'to' 'tradition' 'traditional' 'triangle' 'unesco' 'unique' 'unmatched'\n",
            " 'vibrant' 'views' 'visit' 'visitors' 'wayanad' 'wildlife' 'with'\n",
            " 'wonders' 'world' 'yamuna']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.09540429 0.         0.         0.07255742 0.\n",
            "  0.09540429 0.         0.09540429 0.39443116 0.         0.07255742\n",
            "  0.11269462 0.         0.09540429 0.09540429 0.09540429 0.\n",
            "  0.         0.09540429 0.         0.         0.07255742 0.\n",
            "  0.         0.21767226 0.         0.09540429 0.         0.\n",
            "  0.07255742 0.05634731 0.         0.         0.09540429 0.\n",
            "  0.         0.         0.         0.07255742 0.         0.09540429\n",
            "  0.         0.14511484 0.07255742 0.09540429 0.         0.\n",
            "  0.07255742 0.09540429 0.28621286 0.         0.         0.\n",
            "  0.09540429 0.         0.         0.         0.09540429 0.\n",
            "  0.22538924 0.19080857 0.28621286 0.09540429 0.09540429 0.\n",
            "  0.         0.19080857 0.         0.         0.         0.07255742\n",
            "  0.09540429 0.09540429 0.         0.         0.         0.09540429\n",
            "  0.         0.         0.         0.         0.         0.11269462\n",
            "  0.09540429 0.         0.         0.         0.09540429 0.09540429\n",
            "  0.         0.09540429 0.         0.09540429 0.         0.\n",
            "  0.         0.         0.         0.         0.09540429 0.\n",
            "  0.         0.         0.         0.         0.         0.09540429\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.09540429 0.33808385 0.         0.         0.         0.\n",
            "  0.09540429 0.         0.09540429 0.09540429 0.09540429 0.09540429\n",
            "  0.         0.         0.09540429 0.         0.         0.\n",
            "  0.         0.05634731 0.        ]\n",
            " [0.         0.         0.         0.10891794 0.0828349  0.10891794\n",
            "  0.         0.0828349  0.         0.38597213 0.10891794 0.\n",
            "  0.19298607 0.         0.         0.         0.         0.10891794\n",
            "  0.10891794 0.         0.10891794 0.         0.0828349  0.10891794\n",
            "  0.         0.         0.10891794 0.         0.10891794 0.\n",
            "  0.0828349  0.06432869 0.10891794 0.         0.         0.10891794\n",
            "  0.10891794 0.         0.         0.0828349  0.         0.\n",
            "  0.         0.         0.         0.         0.10891794 0.\n",
            "  0.         0.         0.         0.10891794 0.         0.10891794\n",
            "  0.         0.10891794 0.         0.1656698  0.         0.\n",
            "  0.19298607 0.         0.         0.         0.         0.10891794\n",
            "  0.32675381 0.         0.10891794 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.10891794 0.\n",
            "  0.         0.         0.10891794 0.10891794 0.10891794 0.12865738\n",
            "  0.         0.10891794 0.         0.10891794 0.         0.\n",
            "  0.10891794 0.         0.10891794 0.         0.10891794 0.\n",
            "  0.10891794 0.10891794 0.         0.         0.         0.10891794\n",
            "  0.10891794 0.         0.         0.         0.         0.\n",
            "  0.         0.10891794 0.         0.         0.         0.10891794\n",
            "  0.         0.25731476 0.10891794 0.10891794 0.0828349  0.10891794\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.10891794 0.10891794 0.         0.10891794 0.21783588 0.\n",
            "  0.         0.06432869 0.        ]\n",
            " [0.09700341 0.         0.48501707 0.         0.         0.\n",
            "  0.         0.0737736  0.         0.22916712 0.         0.0737736\n",
            "  0.11458356 0.09700341 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.09700341 0.         0.\n",
            "  0.09700341 0.0737736  0.         0.         0.         0.09700341\n",
            "  0.         0.05729178 0.         0.09700341 0.         0.\n",
            "  0.         0.09700341 0.09700341 0.         0.09700341 0.\n",
            "  0.09700341 0.0737736  0.0737736  0.         0.         0.09700341\n",
            "  0.0737736  0.         0.         0.         0.09700341 0.\n",
            "  0.         0.         0.09700341 0.22132079 0.         0.09700341\n",
            "  0.22916712 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.09700341 0.09700341 0.0737736\n",
            "  0.         0.         0.09700341 0.09700341 0.         0.\n",
            "  0.09700341 0.19400683 0.         0.         0.         0.22916712\n",
            "  0.         0.         0.09700341 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.09700341\n",
            "  0.         0.         0.09700341 0.09700341 0.         0.\n",
            "  0.         0.09700341 0.09700341 0.09700341 0.09700341 0.\n",
            "  0.09700341 0.         0.09700341 0.09700341 0.09700341 0.\n",
            "  0.         0.34375068 0.         0.         0.0737736  0.\n",
            "  0.         0.09700341 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.09700341\n",
            "  0.09700341 0.11458356 0.09700341]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Read the 3 text files (Each file contains 20+ sentences, 150+ words)\n",
        "texts = []\n",
        "for i in range(1, 4):\n",
        "    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:\n",
        "        texts.append(file.read())\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the texts to get the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Display the vocabulary (words from the documents)\n",
        "print(\"Vocabulary (Words):\")\n",
        "print(\"  \" + \"  \".join(tfidf_vectorizer.get_feature_names_out()))  # Words printed horizontally\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF matrix with documents as rows and words as columns\n",
        "print(\"\\nTF-IDF Matrix (Documents vs Words):\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfx0fDYjzcA-",
        "outputId": "ee91cc46-c74a-407d-f475-435da305e251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Words):\n",
            "  adds  advancements  agra  alleppey  also  amazing  amer  an  ancient  and  another  architecture  are  artisans  as  attract  attractions  ayurvedic  backwaters  bazaars  beaches  beauty  blend  breathtaking  circuit  city  coconut  colorful  country  craftsmanship  cuisine  culture  dance  deeply  delicious  destinations  elephants  era  essential  famous  fatehpur  festivals  filled  for  fort  gangaur  god  golden  handicrafts  hawa  heritage  highlight  history  home  hospitality  houseboat  impressive  in  including  india  is  its  jaipur  jantar  jewelry  kathakali  kerala  known  kumarakom  local  love  mahal  mantar  many  marble  markets  mesmerizing  modern  monuments  mughal  munnar  must  nature  of  offer  offers  one  own  palace  perfect  periyar  pink  plantations  popular  provide  renowned  rich  rides  river  rooted  royal  sanctuaries  sanctuary  scenic  seven  significant  sikri  sites  souvenirs  spices  stop  symbol  taj  tea  teej  the  therapies  tigers  to  tradition  traditional  triangle  unesco  unique  unmatched  vibrant  views  visit  visitors  wayanad  wildlife  with  wonders  world  yamuna\n",
            "\n",
            "TF-IDF Matrix (Documents vs Words):\n",
            "       adds  advancements      agra  alleppey      also   amazing      amer  \\\n",
            "0  0.000000      0.095404  0.000000  0.000000  0.072557  0.000000  0.095404   \n",
            "1  0.000000      0.000000  0.000000  0.108918  0.082835  0.108918  0.000000   \n",
            "2  0.097003      0.000000  0.485017  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "         an   ancient       and  ...   vibrant     views     visit  visitors  \\\n",
            "0  0.000000  0.095404  0.394431  ...  0.095404  0.000000  0.000000  0.095404   \n",
            "1  0.082835  0.000000  0.385972  ...  0.000000  0.108918  0.108918  0.000000   \n",
            "2  0.073774  0.000000  0.229167  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "    wayanad  wildlife      with   wonders     world    yamuna  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.056347  0.000000  \n",
            "1  0.108918  0.217836  0.000000  0.000000  0.064329  0.000000  \n",
            "2  0.000000  0.000000  0.097003  0.097003  0.114584  0.097003  \n",
            "\n",
            "[3 rows x 135 columns]\n"
          ]
        }
      ]
    }
  ]
}