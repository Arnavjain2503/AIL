# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q6aFL_2m9wXnCv5kS3PeU35Bakk9L_7K
"""

import nltk
nltk.download('punkt_tab')

# 22. Full Program
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob

nltk.download('punkt')
nltk.download('stopwords')

# Read text
with open('textfile_22.txt', 'r', encoding='utf-8') as file:
    text = file.read()

# a. Text cleaning (remove punctuation, special characters, numbers)
text = re.sub(r'[^A-Za-z\s]', '', text)
text = re.sub(r'\s+', ' ', text).strip()

# b. Convert to lowercase
text = text.lower()

# c. Tokenization
tokens = word_tokenize(text)

# d. Remove stop words
filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]

# e. Correct misspelled words
corrected_tokens = []
for word in filtered_tokens:
    corrected_word = str(TextBlob(word).correct())
    corrected_tokens.append(corrected_word)

# Display final output
print("Corrected Tokens:")
print(corrected_tokens)

# 23. Full Program
import re
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('wordnet')

# Read text
with open('textfile_23.txt', 'r', encoding='utf-8') as file:
    text = file.read()

# a. Text cleaning
text = re.sub(r'[^A-Za-z\s]', '', text)
text = re.sub(r'\s+', ' ', text).strip()

# b. Convert to lowercase
text = text.lower()

# c. Tokenization
tokens = word_tokenize(text)

# Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in tokens]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]

# d. Create list of 3 consecutive words
trigrams = []
for i in range(len(lemmatized_tokens) - 2):
    trigram = ' '.join(lemmatized_tokens[i:i+3])
    trigrams.append(trigram)

# Display trigrams
print("Trigrams after Lemmatization:")
print(trigrams)

# 24. Full Program
from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Read files
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Merge and split text into words
all_text = ' '.join(texts)
words = list(set(all_text.split()))

# Reshape for sklearn
words_array = np.array(words).reshape(-1, 1)

# One Hot Encoding
encoder = OneHotEncoder(sparse_output=False)
onehot_encoded = encoder.fit_transform(words_array)

# Display
print("Words:")
print(words)
print("\nOne Hot Encoded Matrix:")
print(onehot_encoded)

from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Split all text into words and get unique words (vocabulary)
all_text = ' '.join(texts)
words = list(set(all_text.split()))  # Unique words from the combined text

# Create a mapping of word to column index
word_to_index = {word: index for index, word in enumerate(words)}

# Create a 2D array to hold the one-hot encoding matrix for all documents
onehot_encoded = []

# Loop through each document (text file)
for document in texts:
    # Initialize a list for this document
    onehot_vector = [0] * len(words)

    # Split the document into words and set the corresponding positions to 1
    for word in document.split():
        if word in word_to_index:  # Ensure the word exists in the vocabulary
            onehot_vector[word_to_index[word]] = 1

    # Append the one-hot vector of this document to the matrix
    onehot_encoded.append(onehot_vector)

# Convert the result into a NumPy array for easier handling
onehot_encoded = np.array(onehot_encoded)

# Create a DataFrame for better visualization
onehot_df = pd.DataFrame(onehot_encoded, columns=words)

# Display the vocabulary and the one-hot encoded matrix nicely
print("\nOne-Hot Encoded Matrix:")
print(onehot_df)

from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Merge and split text into words
all_text = ' '.join(texts)
words = list(set(all_text.split()))  # Get unique words (vocabulary)

# Initialize the OneHotEncoder (for document encoding)
encoder = OneHotEncoder(sparse_output=False)

# Split the documents into words (list of words for each document)
documents = [document.split() for document in texts]

# Create a list where each document is represented as a list of words
documents_vectorized = []

# For each document, create a list of word occurrences as one-hot encoding
for document in documents:
    # Create a binary vector where 1 means the word exists in the document, 0 means it does not
    one_hot_vector = [1 if word in document else 0 for word in words]
    documents_vectorized.append(one_hot_vector)

# Convert the result into a NumPy array for easier handling
onehot_encoded_documents = np.array(documents_vectorized)

# Create a DataFrame for better visualization
onehot_df = pd.DataFrame(onehot_encoded_documents, columns=words)

# Display the vocabulary and the one-hot encoded matrix nicely
print("Vocabulary (Words):")
print("  " + "  ".join(words))  # Print the vocabulary words horizontally

print("\nOne-Hot Encoded Matrix (Documents vs Words):")
print(onehot_df)  # Display the one-hot encoded DataFrame

from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_24_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Split all text into words and get unique words (vocabulary)
all_text = ' '.join(texts)
words = list(set(all_text.split()))  # Unique words from the combined text

# Create a mapping of word to column index
word_to_index = {word: index for index, word in enumerate(words)}

# Create a 2D array to hold the one-hot encoding matrix for all documents
onehot_encoded = []

# Loop through each document (text file)
for document in texts:
    # Initialize a list for this document
    onehot_vector = [0] * len(words)

    # Split the document into words and set the corresponding positions to 1
    for word in document.split():
        if word in word_to_index:  # Ensure the word exists in the vocabulary
            onehot_vector[word_to_index[word]] = 1

    # Append the one-hot vector of this document to the matrix
    onehot_encoded.append(onehot_vector)

# Convert the result into a NumPy array for easier handling
onehot_encoded = np.array(onehot_encoded)

# Create a DataFrame for better visualization
onehot_df = pd.DataFrame(onehot_encoded, columns=words)

# Display the vocabulary and the one-hot encoded matrix nicely
print("Vocabulary:")
print("  " + "  ".join(words))  # Print the vocabulary words horizontally

print("\nOne-Hot Encoded Matrix:")

# Loop through each document to print its one-hot encoded values
for i, onehot_vector in enumerate(onehot_encoded):
    print(f"Document {i+1}: ", end="")
    for value in onehot_vector:
        print(f"{value}", end="   ")
    print()  # Newline after each document's encoding

import pandas as pd
import string

def one_hot_encode(text, vocabulary):
    encoded_data = {}
    for word in vocabulary:
        encoded_data[word] = []

    for word in text.split():
        vector = [0] * len(vocabulary)
        if word in vocabulary:
            vector[vocabulary.index(word)] = 1
        for i, vocab_word in enumerate(vocabulary):
            encoded_data[vocab_word].append(vector[i])

    df = pd.DataFrame(encoded_data, index=text.split())
    return df

file1 = open("textfile_24_1.txt", "r").read()
file2 = open("textfile_24_2.txt", "r").read()
file3 = open("textfile_24_3.txt", "r").read()

text = file1 + " " + file2 + " " + file3

print(text)

vocabulary = list(set(text.lower().split()))
vocabulary.sort()


encoded_df = one_hot_encode(text.lower(), vocabulary)

print(encoded_df)

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'textfile_25_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Bag of Words
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# Convert the result to a DataFrame for better readability
bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Display the vocabulary and the BoW matrix nicely
print("\nBag of Words Matrix:")
print(bow_df)

# 26. Full Program
from sklearn.feature_extraction.text import TfidfVectorizer

# Read files
texts = []
for i in range(1, 4):
    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

# Display
print("Vocabulary:")
print(tfidf_vectorizer.get_feature_names_out())
print("\nTF-IDF Matrix:")
print(tfidf_matrix.toarray())

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Read the 3 text files (Each file contains 20+ sentences, 150+ words)
texts = []
for i in range(1, 4):
    with open(f'text_file_26_{i}.txt', 'r', encoding='utf-8') as file:
        texts.append(file.read())

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the texts to get the TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

# Display the vocabulary (words from the documents)
print("Vocabulary (Words):")
print("  " + "  ".join(tfidf_vectorizer.get_feature_names_out()))  # Words printed horizontally

# Create a DataFrame for better readability
df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Display the TF-IDF matrix with documents as rows and words as columns
print("\nTF-IDF Matrix (Documents vs Words):")
print(df)